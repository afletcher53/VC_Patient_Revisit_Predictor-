{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy\n",
    "import numpy as np\n",
    "import pandas\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "from keras.layers import TextVectorization\n",
    "from numba import cuda\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from configuration import FilePaths\n",
    "device = cuda.get_current_device()\n",
    "device.reset()\n",
    "from configuration import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FILE_PATHS = FilePaths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds to ensure reproduceability\n",
    "tf.random.set_seed(Config.seed)\n",
    "np.random.seed(Config.seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(Config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all three CSVs into a pandas data frame (I was provided with a combined_documents.csv, secondary_patient_level_annotations.csv and a secondary_problem_level_annotations.csv)\n",
    "for i in tqdm(range(0, 3), ncols=100, desc=\"Loading data..\"):\n",
    "    raw_df_as_read = pd.read_csv(FILE_PATHS.RAW_DATA)\n",
    "    patient_level_annotations_as_read = pd.read_csv(FILE_PATHS.PATIENT_LEVEL_DATA)\n",
    "    problem_level_annotations_as_read = pd.read_csv(FILE_PATHS.PROBLEM_LEVEL_DATA)\n",
    "print(\"------Loading is completed ------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are given lots of extraeneous information, so lets strip non relavent data. Note that patient ID is unique to each patient. \n",
    "# Firstly we create a copy of the data, create an additional column called Date which contains the date record was made\n",
    "# Then we drop unnessicary columns, and filter all records by the study date (1/1/2019 -> 31/12/2019)\n",
    "\n",
    "raw_df = raw_df_as_read.copy()\n",
    "\n",
    "# Then we split the recorded date to DateTimeDay column, so we can get concat records existing on a single day\"\n",
    "raw_df[[\"DateTimeDay\", \"DateTimeSeconds\"]] = raw_df[\"RecordedDate\"].str.split(\n",
    "    \"T\", expand=True\n",
    ")\n",
    "raw_df[\"Date\"] = pd.to_datetime(raw_df[\"DateTimeDay\"], format=\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "raw_df.drop(Config.raw_data_columns_drop, axis=1, inplace=True, errors='ignore') #Drop un-needed columns\n",
    "raw_df = raw_df[(raw_df[\"Date\"] >= Config.study_start_date) & (raw_df[\"Date\"] <= Config.study_end_date)] #Only include patients withint study start date\n",
    "raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_level_annotations = patient_level_annotations_as_read.copy()\n",
    "\n",
    "patient_level_annotations.drop(Config.patient_level_columns_drop, axis=1, inplace=True)\n",
    "patient_level_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_patients = set(patient_level_annotations.loc[patient_level_annotations['Is this patient included in the study_1'] == 'Yes', 'PatientID'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_level_annotations = problem_level_annotations_as_read.copy()\n",
    "\n",
    "# Drop all patients NOT in study\n",
    "problem_level_annotations  =  problem_level_annotations[problem_level_annotations.PatientID.isin(study_patients)]\n",
    "\n",
    "problem_level_annotations[[\"DateTimeDay\", \"DateTimeSeconds\"]] = problem_level_annotations[\"DocumentDate\"].str.split(\n",
    "    \" \", expand=True\n",
    ")\n",
    "problem_level_annotations[\"Date\"] = pd.to_datetime(problem_level_annotations[\"DateTimeDay\"], format=\"%d/%m/%Y\")\n",
    "\n",
    "problem_level_annotations.drop(Config.problem_level_columns_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_multiple_entries(df)->list:\n",
    "    \"Check there are not multiple entries per day, if so remove these days from the data pool \"\n",
    "    multiple_entries_list = []\n",
    "    for i in set(df.PatientID.tolist()):\n",
    "        results = df.loc[df.PatientID ==i]\n",
    "        if not results.Date.is_unique:\n",
    "            multiple_entries_list.append(i)\n",
    "    return multiple_entries_list\n",
    "\n",
    "df = problem_level_annotations[~problem_level_annotations.PatientID.isin(check_multiple_entries(problem_level_annotations))]\n",
    "\n",
    "if not len(check_multiple_entries(df)) == 0:\n",
    "    raise Exception(\"Sorry, but it appears you have multiple categorised days!\")\n",
    "else:\n",
    "    print(\"Data looks good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"You have {len(df.PatientID.unique())} patients included in this study\")\n",
    "print(f\"and a total of {len(df.index)} 24 hour periods classified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge the EHRs to the DF\n",
    "ehrs = []\n",
    "for index, row in df.iterrows():\n",
    "    ehr = raw_df[(raw_df.PatientId ==row.PatientID) & (raw_df.Date==row.Date)]\n",
    "    ehr_list = ehr.Document.to_list()\n",
    "    s = ' '.join(ehr_list)\n",
    "    ehrs.append(s)\n",
    "    \n",
    "df.loc[:,'ehr'] = ehrs.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter only the patient visits\n",
    "df_patients_visits = df[df[Config.is_visit_column] == 'Yes'].copy()\n",
    "patient_id_set:set = set(df_patients_visits[Config.patient_id_column].values)\n",
    "\n",
    "tmp_list: list = []\n",
    "\n",
    "print(f\"{len(patient_id_set)} patients included in this study who had a visit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add a day of the year column in for further formatting. \n",
    "\n",
    "for patient_id in patient_id_set:\n",
    "    patient_stream: list = []\n",
    "    print(patient_id)\n",
    "    print(df_patients_visits.loc[df_patients_visits[Config.patient_id_column] == patient_id])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_patients_visits['PatientID' == 977030]\n",
    "df_patein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patients_visits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d6f07f10bbd904384622e2c81da346bac6398da26490ea76cc729ffb1c8c49fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
